{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e2da19-aa0c-4316-ab4f-a4f5d6f39093",
   "metadata": {},
   "source": [
    "# Generate plots causal tracing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7574ba1-72cb-4c15-bde0-964cca02cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_0=\"\"\"As a customer writing a review, I initially composed the following feedback: \"{}\"\n",
    "After carefully considering the facts, I selected a star rating from the options of \"1\", \"2\", \"3\", \"4\", or \"5\". My final rating was:\"\"\"\n",
    "c1_1=\"\"\"As a customer sharing my experience, I crafted the following review: \"{}\"\n",
    "Taking into account the details of my experience, I chose a star rating from the available options of \"1\", \"2\", \"3\", \"4\", or \"5\". My ultimate rating is:\"\"\"\n",
    "c1_2=\"\"\"As a client providing my opinion, I penned down the subsequent evaluation: \"{}\"\n",
    "Upon thorough reflection of my encounter, I picked a star rating among the choices of \"1\", \"2\", \"3\", \"4\", or \"5\". My conclusive rating stands at:\"\"\"\n",
    "c1_3=\"\"\"As a patron expressing my thoughts, I drafted the ensuing commentary: \"{}\"\n",
    "After meticulously assessing my experience, I opted for a star rating from the range of \"1\", \"2\", \"3\", \"4\", or \"5\". My definitive rating turned out to be:\"\"\"\n",
    "c1_4=\"\"\"As a consumer conveying my perspective, I authored the following assessment: \"{}\"\n",
    "By carefully weighing the aspects of my interaction, I determined a star rating from the possibilities of \"1\", \"2\", \"3\", \"4\", or \"5\". My final verdict on the rating is:\"\"\"\n",
    "\n",
    "c2_0=\"\"\"As a customer writing a review, I initially selected a star rating from the options \"1\", \"2\", \"3\", \"4\", and \"5\", and then provided the following explanations in my review: \"{}\"\n",
    "The review clarifies why I gave a rating of\"\"\"\n",
    "c2_1=\"\"\"As a customer sharing my experience, I first chose a star rating from the available choices of \"1\", \"2\", \"3\", \"4\", and \"5\", and subsequently elaborated on my decision with the following statement: \"{}\"\n",
    "The review elucidates the reasoning behind my assigned rating of\"\"\"\n",
    "c2_2=\"\"\"As a client providing my opinion, I initially picked a star rating from the range of \"1\" to \"5\", and then proceeded to justify my selection with the following commentary: \"{}\"\n",
    "The review sheds light on the rationale for my given rating of\"\"\"\n",
    "c2_3=\"\"\"As a patron expressing my thoughts, I started by selecting a star rating from the scale of \"1\" to \"5\", and then offered an explanation for my choice in the following review text: \"{}\"\n",
    "The review expounds on the basis for my designated rating of\"\"\"\n",
    "c2_4=\"\"\"As a consumer conveying my perspective, I began by opting for a star rating within the \"1\" to \"5\" spectrum, and then detailed my reasoning in the subsequent review passage: \"{}\"\n",
    "The review delineates the grounds for my conferred rating of\"\"\"\n",
    "\n",
    "c0_0=\"\"\"You are an experienced and responsible data annotator for natural language processing (NLP) tasks. In the following, you will annotate some data for sentiment classification. Specifically, given the task description and the review text, you need to annotate the sentiment in terms of \"1\" (most negative), \"2\", \"3\", \"4\", and \"5\" (most positive).\n",
    "Review Text: \"{}\"\n",
    "Sentiment:\"\"\"\n",
    "c0_1=\"\"\"As a proficient data annotator in natural language processing (NLP), your responsibility is to determine the sentiment of the given review text. Please assign a sentiment value from \"1\" (very negative) to \"5\" (very positive).\n",
    "Review Text: \"{}\"\n",
    "Sentiment Score:\"\"\"\n",
    "c0_2=\"\"\"As a skilled data annotator in the field of natural language processing (NLP), your task is to evaluate the sentiment of the given review text. Please classify the sentiment using a scale from \"1\" (highly negative) to \"5\" (highly positive).\n",
    "Review Text: \"{}\"\n",
    "Sentiment Rating:\"\"\"\n",
    "c0_3=\"\"\"As an expert data annotator for NLP tasks, you are required to assess the sentiment of the provided review text. Kindly rate the sentiment on a scale of \"1\" (extremely negative) to \"5\" (extremely positive).\n",
    "Review Text: \"{}\"\n",
    "Sentiment Evaluation:\"\"\"\n",
    "c0_4=\"\"\"As a proficient data annotator in natural language processing (NLP), your responsibility is to determine the sentiment of the given review text. Please assign a sentiment value from \"1\" (very negative) to \"5\" (very positive).\n",
    "Review Text: \"{}\"\n",
    "Sentiment Assessment:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73c5fbd8-2aa0-4f6c-88b1-55abf7d4dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else:\n",
    "            return super().find_class(module, name)\n",
    "\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer,AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7aadc5-0dac-444e-8ce2-54ea0900f322",
   "metadata": {},
   "source": [
    "## get groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c3706d-5cb7-4fcd-988f-5dcba62d0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"/llama-7b-hf\")\n",
    "df=pd.read_csv(\"./yelp_doc_senti_true.csv\")\n",
    "df_sent_tmp=pd.read_csv(\"./yelp_sent_sentiment_tmp.csv\")\n",
    "df_valid_ids=pd.read_csv(\"./yelp_id_text_label.csv\",encoding='utf8')\n",
    "df=df.merge(df_sent_tmp,on='review_id',how='left')\n",
    "df=df_valid_ids.merge(df,on='review_id',how='left')\n",
    "mapping = {0: -1, 1: 0.5, 2: 0, 3: 0.5, 4: 1}\n",
    "df['label_org'] = df['label']+1\n",
    "df['label'] = df['label'].replace(mapping)\n",
    "df.label=df.label*10\n",
    "df=df.assign(label_peak_end_avg_abs_diff=abs(df.label-df.peak_end_avg))\n",
    "df=df.assign(label_all_sent_avg_abs_diff=abs(df.label-df.all_sent_avg))\n",
    "\n",
    "df=df.assign(pred_class=np.where(df.label_peak_end_avg_abs_diff<df.label_all_sent_avg_abs_diff,'C2','C1'))\n",
    "\n",
    "df_ids=df.loc[:,['review_id','pred_class','label_org','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b0a24f-f22b-4abf-ab23-d1baec2e3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=pd.read_csv(\"./yelp_sent_senti_pred.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f739d-b0b7-4dca-8636-9ad0a5dc8f3b",
   "metadata": {},
   "source": [
    "## llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6a7970-1212-4fef-8d09-9f9ff6b1ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the object from the file\n",
    "with open('./pkl_pred/2lay_trace_llama7b.pkl', 'rb') as file:\n",
    "    #res_c1 = pickle.load(file)\n",
    "    contents0 = CPU_Unpickler(file).load()\n",
    "\n",
    "with open('./pkl_pred/2lay_trace_llama7b_72.pkl', 'rb') as file:\n",
    "    #res_c1 = pickle.load(file)\n",
    "    contents1 = CPU_Unpickler(file).load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96380dbd-71c4-42b8-aa51-358423a25875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contents=contents0+contents1\n",
    "\n",
    "dx=pd.DataFrame(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3478df2e-f89c-4dbd-83a6-a7911c1c193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx['low_score']=dx['res'].apply(lambda x: x['low_score'])\n",
    "dx['label']=dx['res'].apply(lambda x: x['answer'][0])\n",
    "\n",
    "dx=dx.merge(df_ids,on='review_id',how='left')\n",
    "\n",
    "dx.label_org=dx.label_org.apply(str)\n",
    "\n",
    "c_values = []\n",
    "for _, row in dx.iterrows():\n",
    "    c_column = row['label']  # Get the column name with the correct value\n",
    "    c_value = row['res']['scores'][' '+str(c_column)]  # Extract the maximum value from the corresponding column\n",
    "    c_values.append(c_value.T)\n",
    "\n",
    "dx['vals']=c_values\n",
    "\n",
    "dx.vals=dx.vals-dx.low_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3915aa25-ce7f-44aa-9edc-dd4518fc4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_groups=[[c0_0,c0_1,c0_2,c0_3,c0_4],[c1_0,c1_1,c1_2,c1_3,c1_4],[c2_0,c2_1,c2_2,c2_3,c2_4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2ff5dcb-709d-4792-93f2-af347b3a8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(sl,l):\n",
    "    results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            results.append((ind,ind+sll-1))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb2ffe6-97ea-4f6d-82c8-939cadffe5f3",
   "metadata": {},
   "source": [
    "## group by sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13179d33-20d8-4cf0-bc70-168e823d327d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_rows=[]\n",
    "for i,d in dx.iterrows():\n",
    "    prompt=prompt_groups[d['prompt_type']][d['prompt_id']].format(d['text'])\n",
    "    tokens=tokenizer.encode(prompt)\n",
    "    array=d['vals']\n",
    "    encoded_sent=tokenizer.batch_encode_plus(list(sentences.loc[sentences.review_id==d['review_id'],:].sentence_text.values))['input_ids']\n",
    "    sent=[]\n",
    "    for k in range(len(encoded_sent)):\n",
    "        el=encoded_sent[k]\n",
    "        offset=0\n",
    "        if k==0:\n",
    "            el=el[3:]\n",
    "            offset=2\n",
    "        elif k==len(encoded_sent)-1:\n",
    "            el=el[1:-1]\n",
    "        else:\n",
    "            el=el[1:]\n",
    "        r=find_sub_list(el,tokens)\n",
    "        computed_val=array[:,r[0][0]-offset:r[0][1]+1].mean(axis=1)\n",
    "        if np.isnan(computed_val).any():\n",
    "            print(i,computed_val)\n",
    "        else:\n",
    "            sent.append(computed_val)\n",
    "    all_rows.append(np.stack(sent).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "323f80bc-f72e-4d39-b915-bb79f4d6e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx['sent_vals']=all_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6115e25-daec-4663-b0e4-25ddaa7809dd",
   "metadata": {},
   "source": [
    "## group by bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b05893f4-3c4d-4c7d-854c-5648f89e35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_array = []\n",
    "std_array=[]\n",
    "num_bins = 10\n",
    "all_arrays=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4888f4d-6e8e-4113-be17-939788c27f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_matrices=[]\n",
    "for j,d in dx.iterrows():\n",
    "    array=d['sent_vals']\n",
    "    heatmap_mat=[]\n",
    "    for sub_array in array:\n",
    "        bins_data=[]\n",
    "        bin_value=None\n",
    "        for bin_index in range(num_bins):\n",
    "            bin_start = bin_index / num_bins\n",
    "            bin_end = (bin_index + 1) / num_bins\n",
    "            normalized_index = np.linspace(0, 1, sub_array.shape[0])\n",
    "            values_in_bin = [sub_array[i] for i in range(sub_array.shape[0]) if bin_start <= normalized_index[i] < bin_end]\n",
    "            try:\n",
    "                bin_value=sum(values_in_bin) / len(values_in_bin)\n",
    "            except:\n",
    "                pass\n",
    "            bins_data.append(bin_value)\n",
    "        #print(bins_data[-1])\n",
    "        heatmap_mat.append(bins_data)\n",
    "        #print(j,\"bin data\",len(bins_data))\n",
    "    #print(len(heatmap_mat))\n",
    "    all_matrices.append(np.array(heatmap_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9c5a5ab-60d3-4571-89bb-bb334f783c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx['grouped_vals']=all_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3a68d9c-5489-41d8-b6b5-caa66970e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matrices=np.array(all_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb55317a-cfd1-4e8c-b924-440180cb6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matrices_mean=all_matrices.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2635418-11cd-4db9-80f3-15c8f3b50277",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt1=dx.loc[dx.prompt_type==1,'grouped_vals'].values\n",
    "pt2=dx.loc[dx.prompt_type==2,'grouped_vals'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1df4403d-7094-4db5-a3ee-0313f3c7f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt1=np.mean(pt1)\n",
    "pt2=np.mean(pt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0333969a-25de-441e-b3bc-f9c1b34db1d1",
   "metadata": {},
   "source": [
    "### Llama 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e16dd0-129f-4254-8dd4-7c63396102d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(5, 3), dpi=200)\n",
    "h1 = ax[0].pcolor(\n",
    "    #all_matrices_mean.T,\n",
    "    pt1.T,\n",
    "    cmap=\"Purples\",\n",
    "    vmin=0,\n",
    ")\n",
    "ax[0].set_title(\"Prompt C1\")\n",
    "ax[0].set_yticks([1.5 + 2*i for i in range(len(pt1.T)//2)])\n",
    "ax[0].set_xticks([0.5 + i for i in range(0, pt1.T.shape[1], 1)])\n",
    "ax[0].set_xticklabels(list(range(0, pt1.T.shape[1], 1)))\n",
    "#ax.set_yticklabels(['0.1','0.2','0.3','0.4','0.5','0.6','0.7','0.8','0.9','1'])\n",
    "ax[0].set_yticklabels(['0.2','0.4','0.6','0.8','1'])\n",
    "#plt.xlabel(\"Layers\")\n",
    "ax[0].set_ylabel(\"Sentence position\")\n",
    "#cb = plt.colorbar(h)\n",
    "ax[0].invert_yaxis()\n",
    "\n",
    "h2 = ax[1].pcolor(\n",
    "    #all_matrices_mean.T,\n",
    "    pt2.T,\n",
    "    cmap=\"Purples\",\n",
    "    vmin=0,\n",
    ")\n",
    "ax[1].set_yticks([1.5 + 2*i for i in range(len(pt2.T)//2)])\n",
    "ax[1].set_xticks([0.5 + i for i in range(0, pt2.T.shape[1], 1)])\n",
    "ax[1].set_xticklabels(list(range(0, pt2.T.shape[1], 1)))\n",
    "#ax.set_yticklabels(['0.1','0.2','0.3','0.4','0.5','0.6','0.7','0.8','0.9','1'])\n",
    "ax[1].set_yticklabels(['0.2','0.4','0.6','0.8','1'])\n",
    "ax[1].set_title(\"Prompt C2\")\n",
    "#ax.set_xlabel(\"Layers\")\n",
    "#plt.ylabel(\"Sentence position\")\n",
    "#cb = plt.colorbar(h)\n",
    "\n",
    "#cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "#cbar = fig.colorbar(ax[1], cax=cax)\n",
    "#cbar.set_label('Colorbar')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(right=0.15)  # Increase space on the right side for the colorbar\n",
    "cbar_ax = fig.add_axes([0.99, 0.18, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "cbar = plt.colorbar(h2, cax=cbar_ax)\n",
    "#cbar.set_label('Colorbar')\n",
    "fig.text(0.5, 0.02, 'Layers', ha='center', va='center')\n",
    "ax[1].invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('causal_trace_llama.pdf', format='pdf', bbox_inches='tight',dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba595a1-a777-441b-8068-67995bdcf39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea272d-a22e-4fdb-9d9d-79459a44b3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
